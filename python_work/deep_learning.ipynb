{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abstract\n",
    "\n",
    "Deep learning, a subset of machine learning and artificial intelligence, is a learning algorithm that mimic how human brain (neurological system) works. It can recognize complex patterns like image, text and sound.\n",
    "\n",
    "As an analogy, in neurological system, a neuron receives input from other neurons or external sources, processes these inputs, and generates an output. In the context of deep learning, a neuron can be seen as a function that takes inputs, weights and bias, to compute linear function ($Z^{[l]}=W^{[l]}A^{[l-1]}+b^{[l]}$) and applies activation function ($g(Z^{[l]})$).\n",
    "\n",
    "\n",
    "### Deep learning vs. Machine learning\n",
    "\n",
    ">Deep learning eliminates some of data pre-processing that is typically involved with machine learning. These algorithms can ingest and process unstructured data, like text and images, and it automates feature extraction, removing some of the dependency on human experts. For example, let’s say that we had a set of photos of different pets, and we wanted to categorize by “cat”, “dog”, “hamster”, et cetera. Deep learning algorithms can determine which features (e.g. ears) are most important to distinguish each animal from another. In machine learning, this hierarchy of features is established manually by a human expert.\n",
    "\n",
    ">Then, through the processes of gradient descent and backpropagation, the deep learning algorithm adjusts and fits itself for accuracy, allowing it to make predictions about a new photo of an animal with increased precision.  \n",
    "\n",
    "> Source: https://www.ibm.com/topics/deep-learning\n",
    "\n",
    "\n",
    "### Forward and Backward Propagation\n",
    "\n",
    "Forward propagation involves passing input data through a neural network to obtain predictions.\n",
    "Backward propagation computes gradients of model parameters with respect to a loss function.\n",
    "By adjusting weights based on gradients, the model gradually improves its predictions.\n",
    "\n",
    "\n",
    "* **Forward propagation** &nbsp; $A^{[l-1]}, W^{[l]}, b^{[l]} \\rightarrow Z^{[l]}, A^{[l]}$\n",
    "\n",
    "$$Z^{[l]} = W^{[l]} A^{[l-1]} + b^{[l]}$$\n",
    "\n",
    "$$A^{[l]} = g^{[l]} (Z^{[l]})$$\n",
    "\n",
    "* **Backward propagation** &nbsp; $dA^{[l]} \\rightarrow dA^{[l-1]},dW^{[l]}, db^{[l]}$\n",
    "\n",
    "$$dZ^{[l]} = dA^{[l]} * {g^{[l]}}^{'}(Z^{[l]})$$\n",
    "\n",
    "$$dW^{[l]} = \\frac{1}{m}dZ^{[l]}{A^{[l-1]}}^T$$\n",
    "\n",
    "$$db^{[l]} = \\frac{1}{m}np.sum(dZ^{[l]}, axis=1, keepdims=True)$$\n",
    "\n",
    "$$dA^{[l-1]} = {W^{[l]}}^T dZ^{[l]} = \\frac{dJ}{dA^{[l-1]}} = \\frac{dZ^{[l]}}{dA^{[l-1]}} \\frac{dJ}{dZ^{[l]}} = \\frac{dZ^{[l]}}{dA^{[l-1]}} dZ^{[l]}$$\n",
    "\n",
    "$$, where \\hspace{3mm} dZ^{[L]} = A^{[L]}-Y$$\n",
    "\n",
    "\n",
    "Also, we have a Loss function $L = -YlogA -(1-Y)log(1-A)$\n",
    "\n",
    "\n",
    "<img src=\"img/2layerNN_kiank.png\" style=\"width:650px;height:400px;\">\n",
    "<caption><center> <u>Figure 2</u>: 2-layer neural network. <br> The model can be summarized as: ***INPUT -> LINEAR -> RELU -> LINEAR -> SIGMOID -> OUTPUT***. </center></caption>\n",
    "\n",
    "\n",
    "<img src=\"img/LlayerNN_kiank.png\" style=\"width:650px;height:400px;\">\n",
    "<caption><center> <u>Figure 3</u>: L-layer neural network. <br> The model can be summarized as: ***[LINEAR -> RELU] $\\times$ (L-1) -> LINEAR -> SIGMOID***</center></caption>\n",
    "\n",
    "### Objective\n",
    "\n",
    "In the following Jupyter notebook, I go through what I've learned from Andrew Ng's deep learning specialization course.\n",
    "\n",
    "I revisit the model for training L-layer deep neural network that can identify cats as binary ouput of 0(non-cat) and 1(cat). The training and test data are provided from the lecture. Here are the list of methods that I'm going to implement:\n",
    "\n",
    "```python\n",
    "# 1. initialize\n",
    "def initialize_parameters_deep(layer_dims):\n",
    "  return parameters\n",
    "\n",
    "# 2. linear forward\n",
    "def relu(Z):\n",
    "  return A, cache\n",
    "\n",
    "def sigmoid(Z):\n",
    "  return A, cache\n",
    "\n",
    "def linear_forward(A, W, b):\n",
    "  return Z, cache\n",
    "\n",
    "# 3. activation forward\n",
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "  return A, cache\n",
    "\n",
    "# 4. nn forward\n",
    "def L_model_forward(X, parameters):\n",
    "  return AL, caches\n",
    "\n",
    "# 5. compute cost\n",
    "def compute_cost(AL, Y):\n",
    "  return cost\n",
    "\n",
    "# 6. backward propagation\n",
    "# dZ = dA * g'(Z) where g(Z) = relu\n",
    "def relu_backward(dA, cache):\n",
    "  return dZ\n",
    "\n",
    "# dZ = dA * g'(Z) where g(Z) = sigmoid,\n",
    "# g'(Z) = g(Z)(1- g(Z))\n",
    "def sigmoid_backward(dA, cache):\n",
    "  return dZ\n",
    "\n",
    "def linear_backward(dZ, linear_cache):\n",
    "  return dA_prev, dW, db\n",
    "\n",
    "def linear_activation_backward(dA, cache, activation):\n",
    "  return dA_prev, dW, db\n",
    "\n",
    "def compute_dA(A,Y):\n",
    "  return dA\n",
    "\n",
    "def L_model_backward(AL, Y, caches):\n",
    "  return grads\n",
    "\n",
    "# 7. update parameters\n",
    "def update_parameters(parameters, grads, learning_rate):\n",
    "  return parameters\n",
    "\n",
    "# 8. predict\n",
    "def predict(X, y, parameters):\n",
    "  return np.zeros((1,m))\n",
    "```\n",
    "\n",
    "\n",
    "### Related Topics\n",
    "\n",
    "- [Geoffrey Hinton: Will digital intelligence replace biological intelligence?](https://www.youtube.com/watch?v=iHCeAotHZa4)\n",
    "- [Henrik Kniberg: Generative AI in a Nutshell](https://www.youtube.com/watch?v=2IK3DFHRFfw)\n",
    "- [Emergent Garden: Why Neural Networks can learn (almost) anything](https://www.youtube.com/watch?v=0QczhVg5HaI)\n",
    "- [Emergent Garden: Watching Neural Networks Learn](https://www.youtube.com/watch?v=TkwXa7Cvfr8)\n",
    "- [Steve Brunton: Physics Informed Machine Learning](https://www.youtube.com/watch?v=JoFW2uSd3Uo)\n",
    "- [How I became a machine learning practitioner](https://blog.gregbrockman.com/how-i-became-a-machine-learning-practitioner)\n",
    "- [Gavin Uberti - Real-Time AI & The Future of AI Hardware](https://podcasts.apple.com/tw/podcast/gavin-uberti-real-time-ai-the-future-of-ai-hardware/id1154105909?i=1000638288111)\n",
    "- [Michael Royzen: Beating GPT-4 with Open Source Models (Phind)](https://www.youtube.com/watch?v=z1rHPFiY6FA)\n",
    "- [YC Combinator AI startup by college kids (2024)](https://www.youtube.com/watch?v=fmI_OciHV_8)\n",
    "- [Sholto Douglas & Trenton Bricken - How to Build & Understand GPT-7's Mind](https://www.youtube.com/watch?v=UTuuTTnjxMQ)\n",
    "\n",
    "## Reference\n",
    "\n",
    "- [Make Your Own Neural Network](https://www.amazon.com/Make-Your-Own-Neural-Network/dp/1530826608)\n",
    "- [Neural Networks From Scratch](https://nnfs.io/)\n",
    "- [Understanding Deep Learning](https://udlbook.github.io/udlbook/)\n",
    "- [Deep Learning: Foundations and Concepts](https://bishopbook.com/)\n",
    "- [Zero to Mastery Learn PyTorch for Deep Learning](https://www.learnpytorch.io/)\n",
    "- [LLMs from scratch](https://github.com/rasbt/LLMs-from-scratch)\n",
    "- [Andrej Karpathy, intro to neural networks and backpropagation: building micrograd](https://www.youtube.com/watch?v=VMj-3S1tku0)\n",
    "- [Diffusion models from scratch, from a new theoretical perspective](https://www.chenyang.co/diffusion.html)\n",
    "- [3Blue1Brown: But what is a neural network?](https://www.youtube.com/watch?v=aircAruvnKk)\n",
    "- [3Blue1Brown: Visualizing Attention, a Transformer's Heart](https://www.youtube.com/watch?v=eMlx5fFNoYc)\n",
    "- [Mamba explained](https://thegradient.pub/mamba-explained/)\n",
    "- [LLaMA Now Goes Faster on CPUs](https://justine.lol/matmul/)\n",
    "- [Mamba-Palooza: 90 Days of Mamba-Inspired Research with Jason Meaux: Part 1](https://www.youtube.com/watch?v=Bg1LQ_jWliU)\n",
    "- [Mamba-Palooza: 90 Days of Mamba-Inspired Research with Jason Meaux: Part 2](https://www.youtube.com/watch?v=MwIiQsEVyew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# NOTE: for showing consistent result for each execution\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define activation functions\n",
    "\n",
    "# ReLU is often used for activation function if l=1,...L-1 hidden layers because the derivative is big enough for learning quickly so that the gradient descent converges fast enough to local optimum.\n",
    "def relu(Z):\n",
    "  \"\"\"\n",
    "  z: any vector(numpy) or scalar variable\n",
    "  \"\"\"\n",
    "  A = np.maximum(0,Z)\n",
    "\n",
    "  assert(A.shape == Z.shape)\n",
    "\n",
    "  cache = Z\n",
    "  return A, cache\n",
    "\n",
    "\n",
    "# Sigmoid is often used for activation function when l=L, which is the output layer; \\hat{y} = a^{{L}}. It is because in binary classification problem, we have to have the estimated result \\hat{y} to be between 0 and 1.\n",
    "def sigmoid(Z):\n",
    "  A = 1/(1+np.exp(-Z))\n",
    "  cache = Z\n",
    "\n",
    "  assert(A.shape == Z.shape)\n",
    "\n",
    "  return A, cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters_deep(layer_dims):\n",
    "  \"\"\"\n",
    "  layer_dims is an array of dimension for each layer\n",
    "  l = 1,2,...,L-1\n",
    "  NOTE: in order to use `L_model_forward(X, parameters)`\n",
    "    manually add L^th parameter W_L, b_L into parameters dictionary structure\n",
    "\n",
    "  initialize weight and bias parameters\n",
    "  weight as random matrics with (n_l, n_l-1)\n",
    "  and bias as zero marices (n_l, 1)\n",
    "  \"\"\"\n",
    "\n",
    "  np.random.seed(1)\n",
    "\n",
    "  parameters = {}\n",
    "  L = len(layer_dims)\n",
    "\n",
    "  for l in range(1, L):\n",
    "    parameters[\"W\"+str(l)]= np.random.randn(layer_dims[l], layer_dims[l-1])*.01\n",
    "    parameters[\"b\"+str(l)]= np.zeros((layer_dims[l],1))\n",
    "\n",
    "    assert(parameters[\"W\"+str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
    "    assert(parameters[\"b\"+str(l)].shape == (layer_dims[l], 1))\n",
    "\n",
    "  return parameters\n",
    "\n",
    "\n",
    "def linear_forward(A, W, b):\n",
    "  \"\"\"\n",
    "  A: previous activation\n",
    "  W: weight\n",
    "  b: bias\n",
    "  \"\"\"\n",
    "  Z = np.dot(W,A) + b\n",
    "  cache = (A,W,b)\n",
    "  return Z, cache\n",
    "\n",
    "\n",
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "  \"\"\"\n",
    "  A_prev: previous activation\n",
    "  W: weight\n",
    "  b: bias\n",
    "  \"\"\"\n",
    "  if activation == \"relu\":\n",
    "    Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "    A, activation_cache = relu(Z)\n",
    "  elif activation == \"sigmoid\":\n",
    "    Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "    A, activation_cache = sigmoid(Z)\n",
    "  \n",
    "  cache = (linear_cache, activation_cache)\n",
    "\n",
    "  return A, cache\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [[ 0.01624345 -0.00611756 -0.00528172 -0.01072969  0.00865408]\n",
      " [-0.02301539  0.01744812 -0.00761207  0.00319039 -0.0024937 ]\n",
      " [ 0.01462108 -0.02060141 -0.00322417 -0.00384054  0.01133769]\n",
      " [-0.01099891 -0.00172428 -0.00877858  0.00042214  0.00582815]]\n",
      "b1 = [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "W2 = [[-0.01100619  0.01144724  0.00901591  0.00502494]\n",
      " [ 0.00900856 -0.00683728 -0.0012289  -0.00935769]\n",
      " [-0.00267888  0.00530355 -0.00691661 -0.00396754]]\n",
      "b2 = [[0.]\n",
      " [0.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "layer_dims = [5,4,3]\n",
    "parameters = initialize_parameters_deep(layer_dims)\n",
    "\n",
    "for l in range(1, len(layer_dims)):\n",
    "  print(\"W\"+str(l),\"=\", parameters[\"W\"+str(l)])\n",
    "  print(\"b\"+str(l),\"=\", parameters[\"b\"+str(l)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_forward(X, parameters):\n",
    "  \"\"\"\n",
    "  X input feature vector\n",
    "  parameters is a dictionary data structure with Weights and biases for each layer\n",
    "    l =1,2,...,L-1\n",
    "  \"\"\"\n",
    "  caches = []\n",
    "  A = X\n",
    "  L = len(parameters) // 2\n",
    "\n",
    "  for l in range(1, L):\n",
    "    A_prev = A\n",
    "    A, cache = linear_activation_forward(A_prev,\n",
    "                              parameters[\"W\"+str(l)],\n",
    "                              parameters[\"b\"+str(l)],\n",
    "                              \"relu\")\n",
    "    caches.append(cache)\n",
    "\n",
    "  # NOTE: parameters have to be initialized with additional WL and bL !!!\n",
    "  AL, cache = linear_activation_forward(A,\n",
    "                            parameters[\"W\"+str(L)],\n",
    "                            parameters[\"b\"+str(L)],\n",
    "                            \"sigmoid\")\n",
    "\n",
    "  caches.append(cache)\n",
    "  print(AL.shape)\n",
    "  print(X.shape[1])\n",
    "\n",
    "  assert(AL.shape == (1, X.shape[1]))\n",
    "\n",
    "  return AL, caches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 5)\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "m = 5\n",
    "layer_dims = [4, 3, 3, 1]\n",
    "parameters = initialize_parameters_deep(layer_dims)\n",
    "X = np.random.randn(layer_dims[0], m)\n",
    "\n",
    "AL, caches = L_model_forward(X, parameters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y):\n",
    "  m = Y.shape[1]\n",
    "\n",
    "  cost = -1/m * np.sum(np.multiply(Y, np.log(AL))+ np.multiply(1-Y, np.log(1-AL)))\n",
    "\n",
    "  cost = np.squeeze(cost)\n",
    "\n",
    "  assert(cost.shape == ())\n",
    "\n",
    "  return cost\n",
    "\n",
    "\n",
    "def init_Y(m):\n",
    "  Y = np.random.randint(2, size=(1, m))\n",
    "  return Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compute cost = 0.6931470898640357\n"
     ]
    }
   ],
   "source": [
    "Y = np.random.rand(1, m)\n",
    "\n",
    "print(\"compute cost =\", compute_cost(AL,Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: dZ = dA * g'(Z) where g(Z) = ReLU\n",
    "def relu_backward(dA, cache):\n",
    "  Z = cache\n",
    "  dZ = np.array(dA, copy=True) # just converting dz to a correct object.\n",
    "  \n",
    "  # When z <= 0, you should set dz to 0 as well. \n",
    "  dZ[Z <= 0] = 0\n",
    "  \n",
    "  assert (dZ.shape == Z.shape)\n",
    "  \n",
    "  return dZ\n",
    "\n",
    "# NOTE: dZ = dA * g'(Z) where g(Z) = sigmoid\n",
    "# g'(Z) = g(Z)(1- g(Z)) = s*(1-s) given s=g(Z)\n",
    "def sigmoid_backward(dA, cache):\n",
    "  Z = cache\n",
    "\n",
    "  s = 1/(1+np.exp(-Z))\n",
    "  dZ = dA * s * (1-s)\n",
    "  \n",
    "  assert (dZ.shape == Z.shape)\n",
    "  \n",
    "  return dZ\n",
    "\n",
    "\n",
    "def linear_backward(dZ, linear_cache):\n",
    "  \"\"\"\n",
    "  linear_cache = (A_prev, W, b)\n",
    "  \"\"\"\n",
    "  A_prev, W, b = linear_cache\n",
    "  m = A_prev.shape[1]\n",
    "\n",
    "  dW = 1/m * np.dot(dZ,A_prev.T)\n",
    "  db = 1/m * np.sum(dZ, axis=1, keepdims=True)\n",
    "  dA_prev = np.dot(W.T, dZ)\n",
    "\n",
    "  assert (dA_prev.shape == A_prev.shape)\n",
    "  assert (dW.shape == W.shape)\n",
    "  assert (db.shape == b.shape)\n",
    "\n",
    "  return dA_prev, dW, db\n",
    "\n",
    "\n",
    "def linear_activation_backward(dA, cache, activation):\n",
    "  # linear_cache: (A_prev, W, b)\n",
    "  # activation_cache: (Z)\n",
    "  linear_cache, activation_cache = cache\n",
    "\n",
    "  if activation == \"relu\":\n",
    "    dZ = relu_backward(dA, activation_cache)\n",
    "    dA_prev, dW, db= linear_backward(dZ, linear_cache)\n",
    "  elif activation == \"sigmoid\":\n",
    "    dZ = sigmoid_backward(dA, activation_cache)\n",
    "    dA_prev, dW, db= linear_backward(dZ, linear_cache)\n",
    "\n",
    "  return dA_prev, dW, db\n",
    "\n",
    "\n",
    "def compute_dA(A,Y):\n",
    "  dA = - (np.divide(Y, A) - np.divide(1 - Y, 1 - A))\n",
    "  return dA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.80018618 -0.14358548 -0.65517858 -0.05955494 -1.77837805]]\n",
      "[[ 5.15178950e-03 -4.10914251e-04 -1.87499614e-03 -1.70434879e-04\n",
      "  -5.08937880e-03]\n",
      " [ 4.05757789e-03 -3.23638335e-04 -1.47675732e-03 -1.34235453e-04\n",
      "  -4.00842287e-03]\n",
      " [ 2.26145841e-03 -1.80377224e-04 -8.23058820e-04 -7.48150504e-05\n",
      "  -2.23406226e-03]]\n",
      "[[ 9.54646325e-07 -7.61439845e-08  0.00000000e+00  5.49511373e-07\n",
      "  -9.43081383e-07]\n",
      " [ 1.31801237e-05 -1.05126590e-06  0.00000000e+00  6.54562576e-07\n",
      "  -1.30204547e-05]\n",
      " [-2.48900449e-05  1.98526632e-06  0.00000000e+00 -1.93233858e-06\n",
      "   2.45885176e-05]]\n",
      "[[ 1.55067533e-08 -2.76395962e-09  0.00000000e+00 -5.00281070e-10\n",
      "  -1.12680009e-07]\n",
      " [-5.84011012e-09  1.92446255e-08  0.00000000e+00 -1.02463310e-08\n",
      "   2.99670804e-07]\n",
      " [-5.04217222e-09  1.06841253e-08  0.00000000e+00 -1.68319909e-08\n",
      "  -2.27182426e-07]\n",
      " [-1.02430555e-08 -3.28969710e-08  0.00000000e+00  3.48263183e-08\n",
      "   9.91125999e-08]]\n"
     ]
    }
   ],
   "source": [
    "L = len(layer_dims) -1\n",
    "dAL = compute_dA(AL,Y)\n",
    "print(dAL)\n",
    "\n",
    "dA_prev, dW, db = linear_activation_backward(dAL, caches[L-1], \"sigmoid\")\n",
    "print(dA_prev)\n",
    "dA_prev, dW, db = linear_activation_backward(dA_prev, caches[L-2], \"relu\")\n",
    "print(dA_prev)\n",
    "dA_prev, dW, db = linear_activation_backward(dA_prev, caches[L-3], \"relu\")\n",
    "print(dA_prev)\n",
    "# ....\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_backward(AL, Y, caches):\n",
    "  \"\"\"\n",
    "  Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group\n",
    "  \n",
    "  Arguments:\n",
    "  AL -- probability vector, output of the forward propagation (L_model_forward())\n",
    "  Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n",
    "  caches -- list of caches containing:\n",
    "              every cache of linear_activation_forward() with \"relu\" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)\n",
    "              the cache of linear_activation_forward() with \"sigmoid\" (it's caches[L-1])\n",
    "  \n",
    "  Returns:\n",
    "  grads -- A dictionary with the gradients\n",
    "            grads[\"dA\" + str(l)] = ... \n",
    "            grads[\"dW\" + str(l)] = ...\n",
    "            grads[\"db\" + str(l)] = ... \n",
    "  \"\"\"\n",
    "\n",
    "  grads = {}\n",
    "  L = len(caches) # the number of layers\n",
    "  m = AL.shape[1]\n",
    "  Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
    "\n",
    "  dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "  current_cache = caches[L-1]\n",
    "  grads[\"dA\"+str(L-1)], grads[\"dW\"+str(L)], grads[\"db\"+str(L)] = linear_activation_backward(dAL, current_cache, activation=\"sigmoid\")\n",
    "\n",
    "  # l = L-2, ..., 0\n",
    "  for l in reversed(range(L-1)):\n",
    "    current_cache = caches[l]\n",
    "    dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\"+str(l+1)], current_cache, activation=\"relu\")\n",
    "    grads[\"dA\"+str(l)] = dA_prev_temp\n",
    "    grads[\"dW\"+str(l+1)] = dW_temp\n",
    "    grads[\"db\"+str(l+1)] = db_temp\n",
    "\n",
    "  return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dW1= [[0.41010002 0.07807203 0.13798444 0.10502167]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.05283652 0.01005865 0.01777766 0.0135308 ]]\n",
      "db1= [[-0.22007063]\n",
      " [ 0.        ]\n",
      " [-0.02835349]]\n",
      "dA1= [[ 0.12913162 -0.44014127]\n",
      " [-0.14175655  0.48317296]\n",
      " [ 0.01663708 -0.05670698]]\n"
     ]
    }
   ],
   "source": [
    "from testCases_v2 import *\n",
    "\n",
    "AL, Y_assess, caches = L_model_backward_test_case()\n",
    "grads = L_model_backward(AL, Y_assess, caches)\n",
    "print(\"dW1=\", grads[\"dW1\"])\n",
    "print(\"db1=\", grads[\"db1\"])\n",
    "print(\"dA1=\", grads[\"dA1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "  \"\"\"\n",
    "  Update parameters using gradient descent\n",
    "  \n",
    "  Arguments:\n",
    "  parameters -- python dictionary containing your parameters \n",
    "  grads -- python dictionary containing your gradients, output of L_model_backward\n",
    "  \n",
    "  Returns:\n",
    "  parameters -- python dictionary containing your updated parameters \n",
    "                parameters[\"W\" + str(l)] = ... \n",
    "                parameters[\"b\" + str(l)] = ...\n",
    "  \"\"\"\n",
    "\n",
    "  L = len(parameters) // 2\n",
    "\n",
    "  for l in range(L):\n",
    "    parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * grads[\"dW\" + str(l+1)]\n",
    "    parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * grads[\"db\" + str(l+1)]\n",
    "\n",
    "  return parameters\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [[-0.59562069 -0.09991781 -2.14584584  1.82662008]\n",
      " [-1.76569676 -0.80627147  0.51115557 -1.18258802]\n",
      " [-1.0535704  -0.86128581  0.68284052  2.20374577]]\n",
      "b1 = [[-0.04659241]\n",
      " [-1.28888275]\n",
      " [ 0.53405496]]\n",
      "W2 = [[-0.55569196  0.0354055   1.32964895]]\n",
      "b2 = [[-0.84610769]]\n"
     ]
    }
   ],
   "source": [
    "parameters, grads = update_parameters_test_case()\n",
    "parameters = update_parameters(parameters, grads, 0.1)\n",
    "\n",
    "print (\"W1 = \"+ str(parameters[\"W1\"]))\n",
    "print (\"b1 = \"+ str(parameters[\"b1\"]))\n",
    "print (\"W2 = \"+ str(parameters[\"W2\"]))\n",
    "print (\"b2 = \"+ str(parameters[\"b2\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
